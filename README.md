# ğŸ“Š Hyperparameter Optimization for Supervised Learning Models

## ğŸ§  Project Overview
This project focuses on optimizing hyperparameters in supervised learning models, particularly **XGBoost**, using **Operations Research (OR) techniques**. The goal is to **minimize the Mean Squared Error (MSE)** on the **California Housing Dataset** while comparing the efficiency of various optimization strategies.

The main methods used in this project include:
- âœ… **Gaussian Process Regression (GPR)** as a surrogate model
- âœ… **L-BFGS-B Optimization** for fine-tuning
- âœ… **Bayesian Optimization**, **Grid Search**, and **Random Search** for benchmarking

The project highlights how **OR methods** can enhance hyperparameter optimization beyond traditional techniques.

ğŸ“– **For more details, read the full report in** [`docs/report_v_fr.pdf`](docs/report_v_fr.pdf).

---

## ğŸ“‚ Table of Contents
- [ğŸ“Š Project Overview](#-project-overview)
- [ğŸ“ Dataset](#-dataset)
- [âš™ï¸ Methods Used](#ï¸-methods-used)
- [ğŸ“ˆ Evaluation Metrics](#-evaluation-metrics)
- [ğŸ’» How to Run](#-how-to-run)
- [ğŸ“‚ Results](#-results)
- [ğŸ”¬ Key Insights](#-key-insights)
- [ğŸ’¡ Future Work](#-future-work)
- [ğŸ“œ License](#-license)

---

## ğŸ“ Dataset
The project uses the **California Housing Dataset** from `sklearn.datasets`:
```python
from sklearn.datasets import fetch_california_housing
```
- **Features**: 8 numeric features related to housing (e.g., median income, house age).
- **Target**: Median house value in California districts.
- **Total Samples**: ~20,000
- No external dataset download is required; it is fetched directly using scikit-learn.

---

## âš™ï¸ Methods Used

### ğŸ† Hyperparameter Optimization Techniques
- **Gaussian Process Regression (GPR)**
  - Used as a surrogate model to approximate the objective function.
  - Helps in exploring the hyperparameter space more effectively.
- **L-BFGS-B Optimization**
  - A bounded optimization algorithm used for fine-tuning hyperparameters after GPR.
  - Efficient for large-scale problems with box constraints.
- **Baseline Methods for Comparison**
  - Grid Search
  - Random Search
  - Bayesian Optimization (`skopt.BayesSearchCV`)

### ğŸ“‹ Hyperparameters Optimized
- `n_estimators` (50â€“300)
- `max_depth` (3â€“10)
- `min_child_weight` (1â€“5)
- `learning_rate` (0.01â€“0.3)
- `subsample` (0.6â€“1.0)

---

## ğŸ“ˆ Evaluation Metrics
- **Mean Squared Error (MSE)**: The primary metric used for evaluation.
- **Computation Time**: Measured to compare the efficiency of different optimization techniques.

---

## ğŸ’» How to Run

### âœ… 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Salwa08/OR-Hyperparameter-Optimization.git
cd OR-Hyperparameter-Optimization
```

### âœ… 2ï¸âƒ£ Set Up the Environment
Itâ€™s recommended to use a virtual environment:
```bash
python -m venv venv
# Activate the environment
# Windows:
.\venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### âœ… 3ï¸âƒ£ Run the Jupyter Notebook
```bash
jupyter notebook notebooks/hyperparameter_optimization.ipynb
```
Run the cells in order to:
- Load the dataset
- Initialize the optimizer
- Perform hyperparameter tuning
- Generate visualizations

---

## ğŸ“‚ Results
- **Best MSE Achieved**: 0.1899
- **Optimized Hyperparameters**: Stored in `results/optimization_results.json`
- **Visualizations Generated**:
  - MSE Evolution over iterations
  - Hyperparameter Distributions
  - Correlations between hyperparameters and MSE

### Sample JSON Output:
```json
{
    "OR_Optimization": {
        "MSE": 0.18999189381099146,
        "Time": 147.54119777679443,
        "Parameters": {
            "n_estimators": 192,
            "max_depth": 5,
            "min_child_weight": 1,
            "learning_rate": 0.10483060403018869,
            "subsample": 0.7406229768820679
        }
    }
}
```

---

## ğŸ”¬ Key Insights
- Gaussian Process + L-BFGS-B outperformed basic Grid Search in both efficiency and MSE reduction.
- The Bayesian Optimization method closely followed the OR-based approach but required more iterations to converge.
- Certain hyperparameters (`learning_rate` and `subsample`) had a stronger impact on the MSE than others.

---

## ğŸ’¡ Future Work
- ğŸ”„ Apply this OR-based optimization technique to classification tasks.
- ğŸ“Š Integrate more complex surrogate models (e.g., Random Forests, Neural Networks).
- âš¡ Explore distributed optimization methods for larger datasets.

---

## ğŸ“œ License
This project is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for details.
